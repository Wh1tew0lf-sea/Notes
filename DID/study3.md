## 线性回归模型

*线性回归*（linear regression）可以追溯到19世纪初， 它在回归的各种标准工具中最简单而且最流行。 线性回归基于几个简单的假设： 首先，假设自变量𝑥和因变量𝑦之间的关系是线性的， 即𝑦可以表示为𝑥中元素的加权和，这里通常允许包含观测值的一些噪声； 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。

### 损失函数

线性回归中平方误差函数

在我们开始考虑如何用模型*拟合*（fit）数据之前，我们需要确定一个拟合程度的度量。 *损失函数*（loss function）能够量化目标的*实际*值与*预测*值之间的差距。 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。 回归问题中最常用的损失函数是平方误差函数。 当样本𝑖的预测值为𝑦^(𝑖)，其相应的真实标签为𝑦(𝑖)时， 平方误差可以定义为以下公式：
$$
𝑙^{(𝑖)}(𝑤,𝑏)=12(\hat𝑦^{(𝑖)}−𝑦^{(𝑖)})^2
$$
常数1/2不会带来本质的差别，但这样在形式上稍微简单一些 （因为当我们对损失函数求导后常数系数为1）。 由于训练数据集并不受我们控制，所以经验误差只是关于模型参数的函数。 为了进一步说明，来看下面的例子。 我们为一维情况下的回归问题绘制图像

由于平方误差函数中的二次方项， 估计值𝑦^(𝑖)和观测值𝑦(𝑖)之间较大的差异将导致更大的损失。 为了度量模型在整个数据集上的质量，我们需计算在训练集𝑛个样本上的损失均值（也等价于求和）。
$$
𝐿(𝑤,𝑏)=\frac{1}{𝑛}\sum_{𝑖=1}^𝑛𝑙^{(𝑖)}(𝑤,𝑏)=\frac1{𝑛}\sum_{𝑖=1}^{𝑛}\frac{1}{2}(𝑤^⊤𝑥^{(𝑖)}+𝑏−𝑦^{(𝑖)})^2.
$$


在训练模型时，我们希望寻找一组参数（𝑤∗,𝑏∗）， 这组参数能最小化在所有训练样本上的总损失。如下式：

### 随机梯度下降

##  矢量化加速

##  从线性回归到深度网络

## 实现

## 3.2.1. 生成数据集

为了简单起见，我们将根据带有噪声的线性模型构造一个人造数据集。 我们的任务是使用这个有限样本的数据集来恢复这个模型的参数。 我们将使用低维数据，这样可以很容易地将其可视化。 在下面的代码中，我们生成一个包含1000个样本的数据集， 每个样本包含从标准正态分布中采样的2个特征。 我们的合成数据集是一个矩阵 $$ 𝑋∈𝑅^{1000×2} $$.

我们使用线性模型参数𝑤=[2,−3.4]⊤、𝑏=4.2 和噪声项𝜖生成数据集及其标签：