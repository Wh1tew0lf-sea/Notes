社会科学家邓肯·卢斯于1959年在*选择模型*（choice model）的理论基础上 发明的*softmax函数*正是这样做的： softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持 可导的性质。 为了完成这一目标，我们首先对每个未规范化的预测求幂，这样可以确保输出非负。 为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。如下式：
$$
\hat𝑦=softmax(𝑜)其中\hat{𝑦}_𝑗 = \frac {exp⁡(𝑜_𝑗)}{\sum_𝑘exp⁡(𝑜_𝑘)}
$$


这里，对于所有的𝑗总有0≤𝑦^𝑗≤1。 因此，$$\hat𝑦$$可以视为一个正确的概率分布。 softmax运算不会改变未规范化的预测𝑜之间的大小次序，只会确定分配给每个类别的概率。 因此，在预测过程中，我们仍然可以用下式来选择最有可能的类别。

## 信息论基础

信息论的核心思想是量化数据中的信息内容。 在信息论中，该数值被称为分布𝑃的*熵*（entropy）。可以通过以下方程得到：$$ 
$$
𝐻[𝑃]=\sum_𝑗−𝑃(𝑗)log⁡𝑃(𝑗).
$$
信息论的基本定理之一指出，为了对从分布𝑝中随机抽取的数据进行编码， 我们至少需要𝐻[𝑃]“纳特（nat）”对其进行编码。 “纳特”相当于*比特*（bit），但是对数底为𝑒而不是2。因此，一个纳特是1log⁡(2)≈1.44比特。

## 3.4.8. 模型预测和评估

在训练softmax回归模型后，给出任何样本特征，我们可以预测每个输出类别的概率。 通常我们使用预测概率最高的类别作为输出类别。 如果预测与实际类别（标签）一致，则预测是正确的。 在接下来的实验中，我们将使用*精度*（accuracy）来评估模型的性能。 精度等于正确预测数与预测总数之间的比率。
